{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Module Requirements do a pip install although rasterio and similar are a massive pain on Windows so WSL is advised, or respective libraries in a Conda env\n",
    "rasterio, rioxarray, geopandas, pyrosm, matplotlib, pandas, numpy, scipy, shapely, osmnx, networkx, flatbuffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick hack to fix local imports\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Alfred/CLionProjects/outbreak-sim/python/venv/lib/python3.8/site-packages/geopandas/_compat.py:84: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.8.1-CAPI-1.13.3). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from enum import IntEnum\n",
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "\n",
    "import flatbuffers\n",
    "import pyrosm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import osmnx\n",
    "import networkx as nx\n",
    "from shapely.geometry import mapping, box, LineString\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from rasterio.crs import CRS\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be picked appropriately for the boundary area to avoid distortion, this example is in the UK so using 27700\n",
    "OUT_CRS = CRS.from_epsg(27700)  # https://epsg.io/27700 \n",
    "\n",
    "# used as a folder name for pickles\n",
    "DATASET_NAME = \"london_se_commuter_ring\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Acquire Boundary and Data from OpenStreetMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get OpenStreetMap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 510 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fp = pyrosm.get_data(\"Greater London\",\n",
    "                     directory='./osm_pbf_data'\n",
    "#                      update=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all boundaries using the default settings\n",
    "# osm = pyrosm.OSM(fp)\n",
    "# boundaries = osm.get_boundaries()\n",
    "# boundaries.head(3) # left as an example to see possible Boundary choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundary = osm.get_boundaries(name=\"London Borough of Tower Hamlets\")\n",
    "\n",
    "# bbox = box(-0.04, 51.48, 0.011, 51.52)  # a random box around a part of Tower Hamlets and Canary Wharf\n",
    "# bbox = box(-0.161,51.449,-0.002,51.529)  # a random box containing City of London and some of South London like Brixton\n",
    "# bbox = box(-0.417548,51.370878,0.144128,51.602621) # bigger box of London\n",
    "bbox = box(-0.599,51.179,0.328,51.562) # SE ~quadrant of the London commuter ring\n",
    "# bbox = box(-2.387604,53.398281,-2.112259,53.55031)  # Greater Manchester\n",
    "boundary = geopandas.GeoDataFrame({'geometry': bbox}, index=[0], crs=\"EPSG:4326\")\n",
    "\n",
    "# # Get the shapely geometry from GeoDataFrame\n",
    "bbox_geom = boundary['geometry'].values[0]\n",
    "# Initialise with bounding box\n",
    "# osm = pyrosm.OSM(fp, bounding_box=bbox_geom)\n",
    "osm = pyrosm.OSM(\"./osm_pbf_data/se-england.pbf\", bounding_box=bbox_geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crudely get all places that might be considered workplaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 48.8 s, total: 2min 30s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pois_filter = {\"shop\": True,\"amenity\": True, \"leisure\": True, \"tourism\": True}\n",
    "pois_filter = {\"shop\": True,\"amenity\": True}\n",
    "pois = osm.get_pois(custom_filter=pois_filter)\n",
    "\n",
    "office_filter = {\"office\": True}\n",
    "offices = osm.get_data_by_custom_criteria(custom_filter=office_filter)\n",
    "\n",
    "office_building_filter = {\"building\": [\"office\", \"offices\"]}\n",
    "office_buildings = osm.get_buildings(custom_filter=office_building_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine the locations of all of the crude workplaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 93.8 ms, total: 3.16 s\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trimmed_work = pois[['geometry']].copy()\n",
    "trimmed_work = trimmed_work.append(offices[['geometry']]).append(office_buildings[['geometry']])\n",
    "trimmed_work = trimmed_work.reset_index()\n",
    "orig_crs = trimmed_work.crs\n",
    "trimmed_work = trimmed_work.to_crs(OUT_CRS) # convert CRS to the final CRS for more correct centroids\n",
    "trimmed_work['geometry'] = trimmed_work.geometry.convex_hull.centroid  # we use the convex hull because otherwise we get wildly incorrect points for non-convex polygons results\n",
    "trimmed_work = trimmed_work.to_crs(orig_crs) # convert back for now\n",
    "# trimmed_work.plot(markersize=0.1)\n",
    "del pois, offices, office_buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all residential buildings as well as ones without a specific tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 4.16 s, total: 1min 13s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "residential_filter = {\"building\": [\"residential\", \"apartments\", \"flats\", \"house\", \"yes\"]}\n",
    "residential_buildings = osm.get_buildings(custom_filter=residential_filter)\n",
    "residential_buildings = residential_buildings[['building', 'geometry']]\n",
    "del residential_filter, osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Acquire Population Count for Selected Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "# this can probably be replaced with an API call (https://www.worldpop.org/sdi/introapi) but isn't currently implemented \n",
    "uk_wp = rxr.open_rasterio('../data/gbr_ppp_2020_UNadj_constrained.tif', masked=True).squeeze() # acquired from https://www.worldpop.org/geodata/summary?id=29480\n",
    "print(uk_wp.rio.crs)  # make sure the crs is EPSG:4326 (WGS84) to match the unprojected OSM boundary\n",
    "wp_clipped = uk_wp.rio.clip(boundary.geometry.apply(mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load, Clip, and Transform Public Transit Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.25 s, sys: 172 ms, total: 2.42 s\n",
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "in_crs = \"EPSG:4326\"\n",
    "# get all public transport\n",
    "full_edges = pd.read_csv('../data/uk_aggregate/Data_Release_v1.11/edges.csv')\n",
    "full_nodes = pd.read_csv('../data/uk_aggregate/Data_Release_v1.11/nodes.csv')\n",
    "full_nodes = geopandas.GeoDataFrame(full_nodes, geometry=geopandas.points_from_xy(full_nodes.lon, full_nodes.lat), crs=in_crs)\n",
    "layers = pd.read_csv('../data/uk_aggregate/Data_Release_v1.11/layers.csv')\n",
    "\n",
    "# Clip the nodes to the boundary\n",
    "nodes = geopandas.clip(full_nodes, boundary)\n",
    "# Only select edges that start or end at the clipped nodes\n",
    "des_edges = full_edges[full_edges.rename(columns={'des_node': 'node', 'des_layer': 'layer'}).set_index(['node', 'layer']).index.isin(nodes.set_index(['node', 'layer']).index)]\n",
    "ori_edges = full_edges[full_edges.rename(columns={'ori_node': 'node', 'ori_layer': 'layer'}).set_index(['node', 'layer']).index.isin(nodes.set_index(['node', 'layer']).index)]\n",
    "edges = des_edges.merge(ori_edges) # inner merge to get edges that start and end in the boundary\n",
    "del full_edges, des_edges, ori_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up columns needed to be parsed by OSMNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['osmid'] = nodes['node'].astype(str) + '_' + nodes['layer'].astype(str)\n",
    "nodes['x'] = nodes.geometry.x\n",
    "nodes['y'] = nodes.geometry.y\n",
    "nodes.set_index('osmid', verify_integrity=True, inplace=True, drop=False) # keep the column as osmnx needs it (as well as it being the index of the DF)\n",
    "\n",
    "edges['u'] = edges['ori_node'].astype(str) + '_' + edges['ori_layer'].astype(str)\n",
    "edges['v'] = edges['des_node'].astype(str) + '_' + edges['des_layer'].astype(str)\n",
    "edges['key'] = edges.reset_index()['index']\n",
    "edges['osmid'] = edges['u'].astype(str) + '_' + edges['v'].astype(str)\n",
    "edges.set_index(['u', 'v', 'key'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Line Geometries for the Edges to convert to GDDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 s, sys: 0 ns, total: 15.2 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_index = full_nodes.index\n",
    "full_nodes = full_nodes.set_index(['node', 'layer'])\n",
    "edges = geopandas.GeoDataFrame(edges, geometry=edges.apply(lambda x: LineString([\n",
    "    full_nodes.loc[(x.ori_node, x.ori_layer)].geometry, \n",
    "    full_nodes.loc[(x.des_node, x.des_layer)].geometry\n",
    "]), axis=1))\n",
    "# full_nodes = full_nodes.reset_index()\n",
    "# full_nodes.index = old_index\n",
    "del full_nodes, old_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parse as OSMNX Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = osmnx.utils_graph.graph_from_gdfs(nodes, edges, None)\n",
    "graph_node_ids = osmnx.utils_graph.graph_to_gdfs(graph, edges=False).osmid # the graph module removes some nodes (probably unconnected ones, TODO investigate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05) # rescale the color bar to match the image\n",
    "wp_clipped.plot(ax=ax, cbar_kwargs={'cax': cax})\n",
    "residential_buildings.plot(ax=ax, facecolor='none', edgecolor='r')\n",
    "trimmed_work.plot(ax=ax, markersize=1)\n",
    "osmnx.plot_graph(graph, ax=ax, node_size=5, node_color='orange', edge_linewidth=1, edge_color='orange')\n",
    "print(f\"Total Population: {int(wp_clipped.sum())}\")\n",
    "print(f\"Buildings from OSM, Residential: {len(residential_buildings)}, Workplaces: {len(trimmed_work)}\")\n",
    "print(f\"Transit Nodes: {len(graph_node_ids)}, Edges: {len(edges)}\")\n",
    "del fig, ax, nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Setup some Distributions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_boundary = boundary.copy().set_crs(\"EPSG:4326\").to_crs(OUT_CRS)\n",
    "bounds = projected_boundary.bounds\n",
    "(boundary_minx, boundary_maxx, boundary_miny, boundary_maxy)  = (bounds.loc[bounds.index[0], 'minx'], bounds.loc[bounds.index[0], 'maxx'], bounds.loc[bounds.index[0], 'miny'], bounds.loc[bounds.index[0], 'maxy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://gis.stackexchange.com/a/301935\n",
    "def cKDQueryRadius(gdA_in, gdB_in, radius=300, reproject=True, p=2.0, workers=1):\n",
    "    gdA = gdA_in.copy()\n",
    "    gdB = gdB_in.copy()\n",
    "    if reproject:\n",
    "        in_crs = gdA.crs\n",
    "        gdA = gdA.to_crs(OUT_CRS)\n",
    "        gdB = gdB.to_crs(OUT_CRS)\n",
    "        \n",
    "    nA = np.array(list(gdA.geometry.centroid.apply(lambda x: (x.x, x.y))))\n",
    "    nB = np.array(list(gdB.geometry.centroid.apply(lambda x: (x.x, x.y))))\n",
    "    \n",
    "    btree = cKDTree(nB)\n",
    "    elements_in_radius = btree.query_ball_point(nA, r=radius, p=p, workers=workers)\n",
    "\n",
    "    gdf = pd.concat(\n",
    "        [gdA.reset_index(drop=True),\n",
    "        pd.Series(elements_in_radius, name='Elements in Radius')], axis=1\n",
    "    )\n",
    "    if reproject:\n",
    "        gdf = gdf.to_crs(gdA_in.crs)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_x, num_y = wp_clipped.sizes['x'], wp_clipped.sizes['y']\n",
    "x, y = wp_clipped.rio.transform() * np.meshgrid(np.arange(num_x)+0.5, np.arange(num_y)+0.5)\n",
    "\n",
    "# GeoDataFrame of centres of raster cells, indexed by their respective ravelled index\n",
    "raster_coords = geopandas.GeoDataFrame({'ravelled_index': np.arange(x.size)}, geometry=geopandas.points_from_xy(x.ravel(), y.ravel()), crs=\"EPSG:4326\")\n",
    "residences_in_radius = cKDQueryRadius(raster_coords, residential_buildings)\n",
    "del num_x, num_y, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Household Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial values pretty much completely guessed, should be built off census data\n",
    "lower, upper = 1, 8\n",
    "mu, sigma = 2.2, 0.98\n",
    "\n",
    "household_size_dist = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(household_size_dist.rvs(100_000).astype(int), density=True, bins=40)\n",
    "plt.show()\n",
    "del fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Workplace Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired from https://www.nature.com/articles/nature04795 supplementary information\n",
    "def workplace_size_truncated_power_law(m_max, a, c):\n",
    "    m = np.arange(1, m_max + 1, dtype='float')\n",
    "    pmf = (\n",
    "            (\n",
    "                    (\n",
    "                            ((1 + (m_max / a))\n",
    "                             /\n",
    "                             (1 + (m / a)))\n",
    "                            ** c)\n",
    "                    - 1)\n",
    "            /\n",
    "            ((\n",
    "                    ((1 + (m_max / a)) ** c)\n",
    "                    - 1)))\n",
    "    pmf /= pmf.sum()\n",
    "\n",
    "    return stats.rv_discrete(values=(range(1, m_max + 1), pmf))\n",
    "\n",
    "max_size = 5920\n",
    "workplace_size_dist = workplace_size_truncated_power_law(m_max=max_size, a=5.36, c=1.34)\n",
    "\n",
    "x = np.arange(1, max_size + 1)\n",
    "fig, ax = plt.subplots()\n",
    "y_pdf = workplace_size_dist.pmf(x)\n",
    "y_cdf = workplace_size_dist.cdf(x)\n",
    "ax.set_xscale(value=\"log\")\n",
    "ax.set_yscale(value=\"log\")\n",
    "ax.plot(x, y_pdf, label='pdf')\n",
    "ax.plot(x, y_cdf, label='cdf')\n",
    "ax.legend()\n",
    "del x, fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person():\n",
    "    def __init__(self, uid, household_uid, age, pos):\n",
    "        self.uid = uid\n",
    "        self.household_uid = household_uid\n",
    "        self.age = age\n",
    "        self.pos = pos\n",
    "        \n",
    "\n",
    "people = []\n",
    "\n",
    "def add_new_person(household_uid, pos):\n",
    "    age = random.randint(0, 108)  # Todo update this\n",
    "    \n",
    "    new_person = Person(uid=len(people), household_uid=household_uid, age=age, pos=pos)\n",
    "    people.append(new_person)\n",
    "    \n",
    "    return new_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Household():\n",
    "    def __init__(self, uid, max_inhabitants, pos: Tuple[float, float]):\n",
    "        self.uid = uid\n",
    "        self.inhabitants = 0\n",
    "        self.max_inhabitants = max_inhabitants\n",
    "        self.pos = pos\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'uid': self.uid,\n",
    "            'inhabitants': self.inhabitants,\n",
    "            'max_inhabitants': self.max_inhabitants,\n",
    "            'pos': self.pos,\n",
    "        }\n",
    "    \n",
    "households = []\n",
    "\n",
    "def add_new_household(pos_geometry):\n",
    "    max_inhabitants = household_size_dist.rvs(1)[0]\n",
    "    \n",
    "    new_household = Household(uid=len(households), max_inhabitants=max_inhabitants, pos=pos_geometry)\n",
    "    households.append(new_household)\n",
    "    \n",
    "    return new_household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidenceType(IntEnum):\n",
    "    HOUSE = 0\n",
    "    SMALL_FLATS = 1  # Perhaps a few floors or flats in one building\n",
    "    LARGE_FLATS = 2  # Generally high-rise, shared lifts, etc.\n",
    "\n",
    "\n",
    "# Again basically random numbers, need to be brought in from Census\n",
    "residence_params = {\n",
    "    'max_household_capacity': 7,\n",
    "    ResidenceType.HOUSE: {\n",
    "        'max_households': 2\n",
    "    },\n",
    "    ResidenceType.SMALL_FLATS: {\n",
    "        'max_households': 10\n",
    "    },\n",
    "    ResidenceType.LARGE_FLATS: {\n",
    "        'max_households': 150\n",
    "    }\n",
    "}\n",
    "\n",
    "residential_buildings['residence_type'] = ResidenceType.HOUSE\n",
    "residential_buildings.loc[residential_buildings['building'].isin(['apartments', 'flats']), 'residence_type'] = ResidenceType.SMALL_FLATS\n",
    "residential_buildings.drop('building', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person Generation and Household Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "residences_to_households = defaultdict(lambda: [])\n",
    "residential_buildings = residential_buildings.to_crs(OUT_CRS)\n",
    "\n",
    "# reset lists\n",
    "households = []\n",
    "people = []\n",
    "failures = 0\n",
    "\n",
    "# for each tile of population\n",
    "for row in range(wp_clipped.shape[0]):\n",
    "    for col in range(wp_clipped.shape[1]):\n",
    "        index = (row, col)\n",
    "        people_at_tile = wp_clipped[row, col]\n",
    "        \n",
    "        if np.isnan(people_at_tile):\n",
    "            continue\n",
    "        \n",
    "        # residences near the center of the tile\n",
    "        local_residences_indices = residences_in_radius.loc[np.ravel_multi_index(index, wp_clipped.shape), 'Elements in Radius']\n",
    "        \n",
    "        for _ in range(int(people_at_tile)):\n",
    "            shuffled_indices = np.random.permutation(local_residences_indices)\n",
    "\n",
    "            chosen_household = None\n",
    "\n",
    "            for residence_index in shuffled_indices:\n",
    "                households_at_residence = [households[household_uid] for household_uid in residences_to_households[residence_index]]\n",
    "                possible_households = [household for household in households_at_residence if household.inhabitants < household.max_inhabitants]\n",
    "\n",
    "                if len(possible_households) != 0:\n",
    "                    chosen_household = random.choice(possible_households)\n",
    "                    chosen_household.inhabitants += 1\n",
    "                    break\n",
    "                else:\n",
    "                    building_type = residential_buildings.loc[residence_index, 'residence_type']\n",
    "                    \n",
    "                    if len(households_at_residence) < residence_params[building_type]['max_households']:\n",
    "                        pos = residential_buildings.loc[residence_index].geometry.centroid\n",
    "                        pos = (pos.x, pos.y)\n",
    "                        chosen_household = add_new_household(pos)\n",
    "                        residences_to_households[residence_index].append(chosen_household.uid)\n",
    "                        \n",
    "                        chosen_household.inhabitants += 1\n",
    "                        \n",
    "            else:  # failed to find an available household, or residence to make a new household in, so upgrade a residence \n",
    "                smaller_residences_indices = [residence_index for residence_index in local_residences_indices \n",
    "                                              if residential_buildings.loc[residence_index, 'residence_type'] != ResidenceType.LARGE_FLATS]\n",
    "                \n",
    "                if len(smaller_residences_indices) != 0:\n",
    "                    chosen_residence_index = random.choice(smaller_residences_indices)\n",
    "                    residential_buildings.loc[chosen_residence_index, 'residence_type'] = ResidenceType(residential_buildings.loc[chosen_residence_index, 'residence_type'] + 1)\n",
    "                    \n",
    "                    pos = residential_buildings.loc[chosen_residence_index].geometry.centroid\n",
    "                    pos = (pos.x, pos.y)\n",
    "                    chosen_household = add_new_household(pos)\n",
    "                    residences_to_households[chosen_residence_index].append(chosen_household.uid)\n",
    "                else:\n",
    "                        failures += 1\n",
    "                        continue\n",
    "#                     raise Exception(\"Bugger gotta deal with this\")\n",
    "            add_new_person(chosen_household.uid, chosen_household.pos)    \n",
    "print(f\"Failures: {failures}/{int(wp_clipped.sum())}, Failure Rate: {failures/int(wp_clipped.sum()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workplace Selection and Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate workplace capacities\n",
    "trimmed_work['capacity'] = workplace_size_dist.rvs(size=len(trimmed_work))\n",
    "# reproject workplaces\n",
    "trimmed_work = trimmed_work.to_crs(OUT_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'pickles/{DATASET_NAME}/people_list').write_bytes(pickle.dumps(people))\n",
    "people_df = pd.DataFrame(data=[{'uid': person.uid, 'x': person.pos[0], 'y': person.pos[1], 'age': person.age, 'household_uid': person.household_uid} for person in people])\n",
    "del people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'pickles/{DATASET_NAME}/households_list').write_bytes(pickle.dumps(households))\n",
    "Path(f'pickles/{DATASET_NAME}/people_df').write_bytes(pickle.dumps(people_df))\n",
    "Path(f'pickles/{DATASET_NAME}/workplaces_gdf').write_bytes(pickle.dumps(trimmed_work))\n",
    "Path(f'pickles/{DATASET_NAME}/boundary').write_bytes(pickle.dumps(boundary))\n",
    "Path(f'pickles/{DATASET_NAME}/transit_graph').write_bytes(pickle.dumps(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from pathlib import Path\n",
    "# households = pickle.loads(Path(f'pickles/{DATASET_NAME}/households_list').read_bytes())\n",
    "# people_df = pickle.loads(Path(f'pickles/{DATASET_NAME}/people_df').read_bytes())\n",
    "# trimmed_work = pickle.loads(Path(f'pickles/{DATASET_NAME}/workplaces_gdf').read_bytes())\n",
    "# boundary = pickle.loads(Path(f'pickles/{DATASET_NAME}/boundary').read_bytes())\n",
    "# graph = pickle.loads(Path(f'pickles/{DATASET_NAME}/transit_graph').read_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size = 400 # meters for OUT_CRS\n",
    "y_len = int(math.ceil((boundary_maxy - boundary_miny) / bucket_size))\n",
    "x_len = int(math.ceil((boundary_maxx - boundary_minx) / bucket_size))\n",
    "\n",
    "print(f'y: {y_len}, x: {x_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployed = people_df.loc[(17 <= people_df['age']) & (people_df['age'] <= 67)].copy()\n",
    "print(len(unemployed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployed['bucket_x'] = np.ceil(((unemployed['x'] - boundary_minx) / bucket_size)).astype(int) - 1\n",
    "unemployed['bucket_y'] = np.ceil(y_len - ((unemployed['y'] - boundary_miny) / bucket_size)).astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_buckets():\n",
    "    unemployed_bucket = [[[] for x in range(x_len)] for y in range(y_len)]\n",
    "    for person in unemployed.itertuples():\n",
    "        unemployed_bucket[person.bucket_y][person.bucket_x].append(person.uid)\n",
    "    \n",
    "    return unemployed_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unemployed_bucket = make_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/44865023/how-can-i-create-a-circular-mask-for-a-numpy-array\n",
    "def create_circular_mask(h, w, centre_y=None, centre_x=None, radius=None):\n",
    "    if centre_x is None: # use the middle of the image\n",
    "        centre_x = int(w / 2)\n",
    "    if centre_y is None:\n",
    "        centre_y = int(h / 2)\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(centre_y, centre_x, (w - centre_x), (h - centre_y))\n",
    "\n",
    "    y, x = np.ogrid[-centre_y:(h - centre_y), -centre_x:(w - centre_x)]\n",
    "    mask = (x * x) + (y * y) <= (radius * radius)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoViablePeopleError(Exception):\n",
    "    pass\n",
    "\n",
    "# A generator that takes a center point and a radius, and efficiently finds an unemployed person in that radius \n",
    "def valid_unemployed_within_dist(y, x, dist, cache_size=200):\n",
    "    counts = np.array([list(map(len, row)) for row in unemployed_bucket])\n",
    "    y = y_len - int(math.ceil((y - boundary_miny) / bucket_size)) - 1\n",
    "    x = int(math.ceil((x - boundary_minx) / bucket_size)) - 1\n",
    "    dist = dist / bucket_size\n",
    "    mask = create_circular_mask(h=y_len, w=x_len, centre_y=y, centre_x=x, radius=dist)\n",
    "    \n",
    "    indices = np.argwhere(mask)\n",
    "    if np.sum(counts[mask]) == 0:\n",
    "        raise NoViablePeopleError(f'No viable unemployed people left around (x:{x}, y:{y})')\n",
    "    probabilities = counts[mask] / np.sum(counts[mask])\n",
    "    \n",
    "    chosen_buckets = np.random.choice(np.arange(indices.shape[0]), cache_size, p=probabilities) # a cache of sampled choices\n",
    "    chosen_iter = iter(chosen_buckets)\n",
    "    failures = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            next_bucket_pos = tuple(indices[next(chosen_iter)])\n",
    "        except StopIteration: # used up the cache, repopulate with some more random choices\n",
    "            chosen_buckets = np.random.choice(np.arange(indices.shape[0]), cache_size, p=probabilities)\n",
    "            chosen_iter = iter(chosen_buckets)\n",
    "            continue\n",
    "        try: \n",
    "            next_person = unemployed_bucket[next_bucket_pos[0]][next_bucket_pos[1]].pop()\n",
    "        except IndexError: # no unemployed people in that bucket\n",
    "            failures += 1\n",
    "            if failures > 300: # we had a lot of failures, recalc the probability map\n",
    "                failures = 0\n",
    "                \n",
    "                counts = np.array([list(map(len, row)) for row in unemployed_bucket])\n",
    "                \n",
    "                total_unemployed_left = np.sum(counts[mask])\n",
    "                if total_unemployed_left == 0:\n",
    "                    raise NoViablePeopleError(f'No viable unemployed people left around ({x}, {y})')\n",
    "                \n",
    "                probabilities = counts[mask] / total_unemployed_left\n",
    "                \n",
    "                chosen_buckets = np.random.choice(np.arange(indices.shape[0]), cache_size, p=probabilities)\n",
    "                chosen_iter = iter(chosen_buckets)\n",
    "            continue\n",
    "            \n",
    "        yield next_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "travel_times = dict(nx.all_pairs_dijkstra_path_length(graph, cutoff=90, weight='minutes'))  # {node(str): {target(str): dist(num)}}\n",
    "Path(f'pickles/{DATASET_NAME}/travel_times').write_bytes(pickle.dumps(travel_times))\n",
    "\n",
    "# travel_times = pickle.loads(Path(f'pickles/{DATASET_NAME}/travel_times').read_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_node_ids = osmnx.utils_graph.graph_to_gdfs(graph, edges=False).osmid # the graph module removes some nodes (probably unconnected ones, TODO investigate)\n",
    "nodes = osmnx.utils_graph.graph_to_gdfs(graph, edges=False).set_crs(epsg=4326)\n",
    "transit_node_positions = np.array(list(nodes.loc[graph_node_ids].to_crs(OUT_CRS).geometry.centroid.apply(lambda x: (x.x, x.y))))\n",
    "nodes_to_radius_search = dict(zip(list(graph_node_ids), list(map(lambda pos: valid_unemployed_within_dist(pos[1], pos[0], 5_000, 100), transit_node_positions))))\n",
    "nodeKdTree = cKDTree(data=transit_node_positions)\n",
    "del nodes, transit_node_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_TRAVEL_TIME = 15\n",
    "graph_node_ids_to_index = {node_id: index for index, node_id in enumerate(list(graph_node_ids))}\n",
    "\n",
    "transit_nodes_to_commuting_nodes = {graph_node_ids_to_index[src_node_id]: [target_node_id for target_node_id, time in times_dict.items() if time > MIN_TRAVEL_TIME] \n",
    "                   for src_node_id, times_dict in travel_times.items()}\n",
    "\n",
    "def get_transit_nodes_to_commuting_nodes():\n",
    "    return deepcopy(transit_nodes_to_commuting_nodes)\n",
    "    \n",
    "del travel_times, graph_node_ids_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoNearbyTransitNodesError(Exception):\n",
    "    pass\n",
    "\n",
    "def get_nearby_transit_nodes(workplace):\n",
    "    dist = 1_000\n",
    "    nearby_transit_nodes = nodeKdTree.query_ball_point((workplace.geometry.x, workplace.geometry.y), dist)\n",
    "    while len(nearby_transit_nodes) == 0 and dist <= 5_000:\n",
    "        dist += 1000\n",
    "        nearby_transit_nodes = nodeKdTree.query_ball_point((workplace.geometry.x, workplace.geometry.y), dist)\n",
    "    if len(nearby_transit_nodes) == 0:\n",
    "        raise NoNearbyTransitNodesError(f\"No transit nodes were found within walking distance of workplace {workplace.index}\")\n",
    "    \n",
    "    return nearby_transit_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class TransportType(Enum):\n",
    "    PUBLIC_TRANSIT = 0,\n",
    "    DRIVING = 1,\n",
    "    CYCLING = 2,\n",
    "    WALKING = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "to_allocate = len(unemployed)\n",
    "print(f'Trying to allocate {to_allocate} people, {len(trimmed_work)} workplaces')\n",
    "\n",
    "unemployed_bucket = make_buckets()\n",
    "reachable_nodes = get_transit_nodes_to_commuting_nodes()\n",
    "\n",
    "people_to_workplaces = [None] * len(people_df)\n",
    "people_to_transport_type = [None] * len(people_df)\n",
    "\n",
    "failures_per_transport = {transport_type: 0 for transport_type in TransportType}\n",
    "complete_failures = 0\n",
    "successes = 0\n",
    "workplace_indices = []\n",
    "\n",
    "for index, workplace in enumerate(trimmed_work.sample(frac=1).itertuples()): # iterate over shuffled workplaces\n",
    "    if successes >= to_allocate:\n",
    "        print(f'Allocated all people after {index} workplaces')\n",
    "        break\n",
    "    if index < 9:\n",
    "        workplace_indices.append(workplace.index) # hacky way to select some workplaces for analysis later on\n",
    "        \n",
    "    transport_options = [TransportType.PUBLIC_TRANSIT, TransportType.DRIVING, TransportType.CYCLING, TransportType.WALKING]\n",
    "    \n",
    "    try:\n",
    "        nearby_transit_nodes = get_nearby_transit_nodes(workplace)\n",
    "    except NoNearbyTransitNodesError:\n",
    "        transport_options.remove(TransportType.PUBLIC_TRANSIT)\n",
    "        \n",
    "    valid_unemployed_gen_60k = valid_unemployed_within_dist(workplace.geometry.y, workplace.geometry.x, 60_000)\n",
    "    valid_unemployed_gen_20k = valid_unemployed_within_dist(workplace.geometry.y, workplace.geometry.x, 20_000)\n",
    "    valid_unemployed_gen_5k = valid_unemployed_within_dist(workplace.geometry.y, workplace.geometry.x, 5_000)\n",
    "    \n",
    "    for _ in range(workplace.capacity):\n",
    "        # TODO update to better reflect real distributions if possible, maybe weight it by workplace size, more likely to walk if you own the business\n",
    "        random.shuffle(transport_options)\n",
    "        random_transport = iter(transport_options)\n",
    "        \n",
    "        person_id = None\n",
    "        transport_type = None\n",
    "        \n",
    "        while (person_id == None):\n",
    "            try:\n",
    "                transport_type = next(random_transport)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            if transport_type == TransportType.PUBLIC_TRANSIT:\n",
    "                source_node_index = np.random.choice(nearby_transit_nodes)\n",
    "                try:\n",
    "                    while (person_id == None):\n",
    "                        dest_osmid = random.choice(reachable_nodes[source_node_index])\n",
    "                        try:\n",
    "                            person_id = next(nodes_to_radius_search[dest_osmid])\n",
    "                        except StopIteration:\n",
    "                            reachable_nodes[source_node_index].remove(dest_osmid)\n",
    "                except (NoViablePeopleError, IndexError):\n",
    "                    transport_options.remove(transport_type)\n",
    "                    failures_per_transport[transport_type] += 1\n",
    "            elif transport_type == TransportType.DRIVING:\n",
    "                try:\n",
    "                    person_id = next(valid_unemployed_gen_60k)\n",
    "                except NoViablePeopleError:\n",
    "                    transport_options.remove(transport_type)\n",
    "                    failures_per_transport[transport_type] += 1\n",
    "            elif transport_type == TransportType.CYCLING:\n",
    "                try:\n",
    "                    person_id = next(valid_unemployed_gen_20k)\n",
    "                except NoViablePeopleError:\n",
    "                    transport_options.remove(transport_type)\n",
    "                    failures_per_transport[transport_type] += 1\n",
    "            elif transport_type == TransportType.WALKING:\n",
    "                try:\n",
    "                    person_id = next(valid_unemployed_gen_5k)\n",
    "                    failures_per_transport[transport_type] += 1\n",
    "                except NoViablePeopleError:\n",
    "                    transport_options.remove(transport_type)\n",
    "        \n",
    "        if person_id:\n",
    "            if transport_type:\n",
    "                people_to_transport_type[person_id] = transport_type\n",
    "            \n",
    "            people_to_workplaces[person_id] = workplace.index\n",
    "            successes += 1\n",
    "            if successes >= to_allocate:\n",
    "                break\n",
    "        else:\n",
    "            complete_failures +=1\n",
    "            break            \n",
    "    \n",
    "print(f'Successes: {successes}, Failures: {complete_failures}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_per_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workplaces_to_people = defaultdict(list)\n",
    "for person_id, workplace_index in enumerate(people_to_workplaces):\n",
    "    if workplace_index is not None:\n",
    "        workplaces_to_people[workplace_index].append(person_id)\n",
    "\n",
    "example_workplaces_to_people = {ind: workplaces_to_people[ind] for ind in workplace_indices}\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_aspect('equal')\n",
    "marker_styles = [\n",
    "                ('red', 'o'), ('blue', 'o'), ('green', 'o'),\n",
    "                ('red', 'P'), ('blue', 'P'), ('green', 'P'),\n",
    "                ('red', '*'), ('blue', '*'), ('green', '*')\n",
    "                ]\n",
    "\n",
    "for (workplace_index, worker_indices), (color, marker) in zip(example_workplaces_to_people.items(), marker_styles):\n",
    "    trimmed_work.iloc[[workplace_index]].plot(ax=ax, markersize=100, c=color, marker=marker)\n",
    "    workers = people_df.iloc[worker_indices]\n",
    "    workers.plot(ax=ax, x='x', y='y', kind='scatter', s=30, c=color, marker=marker)\n",
    "del fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and Write Model to FlatBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Translate co-ordinates to have the minima start at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_x_y(x, y):\n",
    "    norm_x = (x - boundary_minx)\n",
    "    norm_y = (y - boundary_miny)\n",
    "    return (norm_x, norm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = osmnx.utils_graph.graph_to_gdfs(graph) # get the nodes and edges dataframes\n",
    "\n",
    "# We need a lookup of an OSM_ID to the node's index as we're using the index as the ID in the Flatbuffer\n",
    "node_osmids_to_index = {osmid: index for index, osmid in enumerate(nodes.index)}\n",
    "\n",
    "nodes = nodes.set_crs(epsg=4326).to_crs(OUT_CRS).reset_index(drop=True)\n",
    "nodes_norm_x, nodes_norm_y = normalise_x_y(x=nodes.geometry.centroid.x, y=nodes.geometry.centroid.y)\n",
    "nodes['pos'] = list(zip(nodes_norm_x, nodes_norm_y))\n",
    "\n",
    "edges = edges.set_crs(epsg=4326).to_crs(OUT_CRS).reset_index()\n",
    "edges_norm_x, edges_norm_y = normalise_x_y(x=edges.geometry.centroid.x, y=edges.geometry.centroid.y)\n",
    "edges['v'] = edges['v'].apply(lambda osm_id: node_osmids_to_index[osm_id])\n",
    "edges['u'] = edges['u'].apply(lambda osm_id: node_osmids_to_index[osm_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocated_workplaces = list(workplaces_to_people.keys())\n",
    "workplaces = trimmed_work.iloc[allocated_workplaces].copy()\n",
    "\n",
    "# We need a lookup of a workplace index within the full dataframe to its index out of allocated workplaces\n",
    "workplaces_old_index_to_new = {old_index: index for index, old_index in enumerate(workplaces.index)}\n",
    "workplaces_old_index_to_new[2 ** 32 - 1] = 2 ** 32 - 1 # set the uint32 MAX to stay the same\n",
    "\n",
    "workplaces = workplaces.to_crs(OUT_CRS).reset_index(drop=True)\n",
    "\n",
    "workplaces_norm_x, workplaces_norm_y = normalise_x_y(x=workplaces.geometry.centroid.x, y=workplaces.geometry.centroid.y)\n",
    "workplaces['pos'] = list(zip(workplaces_norm_x, workplaces_norm_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_to_workplace_ids = [2 ** 32 - 1] * len(people_df)  # Initial value of uint32 MAX\n",
    "for old_workplace_id, employee_ids in workplaces_to_people.items():\n",
    "    for employee_id in employee_ids:\n",
    "        people_to_workplace_ids[employee_id] = workplaces_old_index_to_new[old_workplace_id]\n",
    "\n",
    "people_df['workplace_uid'] = people_to_workplace_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "households_norm_pos = [normalise_x_y(household.pos[0], household.pos[1]) for household in households]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate bounds with padding from all input data to account for discrepancies in co-ordinate transforms\n",
    "position_lists = [workplaces['pos'], nodes['pos'], households_norm_pos]\n",
    "max_x = 0\n",
    "max_y = 0\n",
    "for pos_list in position_lists:\n",
    "    max_x = max(max_x, max(list(pos_list), key=itemgetter(0))[0])\n",
    "    max_y = max(max_y, max(list(pos_list), key=itemgetter(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, nonsense atm\n",
    "# All in bytes\n",
    "AGENT_SIZE = 32\n",
    "WORKPLACE_SIZE = 8\n",
    "HOUSEHOLD_SIZE = 8\n",
    "CONTAINER_SIZE = 8\n",
    "TRANSIT_NODE_SIZE = 8\n",
    "TRANSIT_EDGE_SIZE = 16\n",
    "\n",
    "initial_buffer_size = (len(people_df) * AGENT_SIZE\n",
    "                       + len(households) * HOUSEHOLD_SIZE\n",
    "                       + len(workplaces_to_people) * WORKPLACE_SIZE\n",
    "                       + len(nodes) * TRANSIT_NODE_SIZE\n",
    "                       + len(edges) * TRANSIT_EDGE_SIZE\n",
    "                      )\n",
    "print(initial_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generated.OutbreakSim.Model.Vec2 as gen_vec2\n",
    "import generated.OutbreakSim.Model.Bounds as gen_bounds\n",
    "import generated.OutbreakSim.Model.Model as gen_model\n",
    "import generated.OutbreakSim.Model.Agents as gen_agents\n",
    "import generated.OutbreakSim.Model.Households as gen_households\n",
    "import generated.OutbreakSim.Model.Workplaces as gen_workplaces\n",
    "import generated.OutbreakSim.Model.TransitNode as gen_transit_node\n",
    "import generated.OutbreakSim.Model.TransitEdge as gen_transit_edge\n",
    "import generated.OutbreakSim.Model.TransitGraph as gen_transit_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = flatbuffers.Builder(initial_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create `vectors` representing the attributes of the Agents SoA\n",
    "# Note: Since we prepend the bytes, this loop iterates in reverse.\n",
    "\n",
    "num_people = len(people_df)\n",
    "gen_agents.AgentsStartAgeVector(builder, num_people)\n",
    "for age in people_df['age'][::-1]:\n",
    "    builder.PrependUint8(int(age))\n",
    "agents_age_vec = builder.EndVector(num_people)\n",
    "\n",
    "gen_agents.AgentsStartHouseholdIndexVector(builder, num_people)\n",
    "for household_uid in people_df['household_uid'][::-1]:\n",
    "    builder.PrependUint32(int(household_uid))\n",
    "agents_household_index_vec = builder.EndVector(num_people)\n",
    "\n",
    "gen_agents.AgentsStartWorkplaceIndexVector(builder, num_people)\n",
    "for workplace_uid in people_df['workplace_uid'][::-1]:\n",
    "    builder.PrependUint32(int(workplace_uid))\n",
    "agents_workplace_index_vec = builder.EndVector(num_people)\n",
    "\n",
    "gen_agents.AgentsStart(builder)\n",
    "gen_agents.AgentsAddAge(builder, agents_age_vec)\n",
    "gen_agents.AgentsAddHouseholdIndex(builder, agents_household_index_vec)\n",
    "gen_agents.AgentsAddWorkplaceIndex(builder, agents_workplace_index_vec)\n",
    "built_agents = gen_agents.AgentsEnd(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create `vectors` representing the attributes of the Households SoA\n",
    "# Note: Since we prepend the bytes, this loop iterates in reverse.\n",
    "gen_households.HouseholdsStartPosVector(builder, len(households))\n",
    "for household_pos in reversed(households_norm_pos):\n",
    "    gen_vec2.CreateVec2(builder, household_pos[0], household_pos[1])\n",
    "households_pos_vec = builder.EndVector(len(households))\n",
    "\n",
    "gen_households.HouseholdsStart(builder)\n",
    "gen_households.HouseholdsAddPos(builder, households_pos_vec)\n",
    "built_households = gen_households.HouseholdsEnd(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create `vectors` representing the attributes of the Workplaces SoA\n",
    "# Note: Since we prepend the bytes, this loop iterates in reverse.\n",
    "num_workplaces = len(workplaces)\n",
    "gen_workplaces.WorkplacesStartPosVector(builder, num_workplaces)\n",
    "for pos in workplaces['pos'][::-1]:\n",
    "    gen_vec2.CreateVec2(builder, pos[0], pos[1])\n",
    "workplaces_pos_vec = builder.EndVector(len(households))\n",
    "\n",
    "gen_workplaces.WorkplacesStart(builder)\n",
    "gen_workplaces.WorkplacesAddPos(builder, workplaces_pos_vec)\n",
    "built_workplaces = gen_workplaces.WorkplacesEnd(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "num_nodes = len(nodes)\n",
    "gen_transit_graph.TransitGraphStartNodesVector(builder, num_nodes)\n",
    "for pos in nodes['pos'][::-1]:\n",
    "    gen_transit_node.CreateTransitNode(builder, pos[0], pos[1])\n",
    "transit_graph_nodes_vec = builder.EndVector(num_nodes)\n",
    "\n",
    "num_edges = len(edges)\n",
    "gen_transit_graph.TransitGraphStartEdgesVector(builder, num_edges)\n",
    "for row in edges[['u', 'v', 'minutes']][::-1].itertuples():\n",
    "    gen_transit_edge.CreateTransitEdge(builder, row.u, row.v, row.minutes)\n",
    "transit_graph_edges_vec = builder.EndVector(num_edges)\n",
    "\n",
    "gen_transit_graph.TransitGraphStart(builder)\n",
    "gen_transit_graph.TransitGraphAddNodes(builder, transit_graph_nodes_vec)\n",
    "gen_transit_graph.TransitGraphAddEdges(builder, transit_graph_edges_vec)\n",
    "built_transit_graph = gen_transit_graph.TransitGraphEnd(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "gen_model.ModelStart(builder)\n",
    "built_bounds = gen_bounds.CreateBounds(builder, 0, 0, boundary_maxx - boundary_minx, boundary_maxy - boundary_miny)\n",
    "gen_model.ModelAddBounds(builder, built_bounds)\n",
    "gen_model.ModelAddAgents(builder, built_agents)\n",
    "gen_model.ModelAddHouseholds(builder, built_households)\n",
    "gen_model.ModelAddWorkplaces(builder, built_workplaces)\n",
    "gen_model.ModelAddTransitGraph(builder, built_transit_graph)\n",
    "built_model = gen_model.ModelEnd(builder)\n",
    "builder.Finish(built_model)\n",
    "buf = builder.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(f\"./output/model_{DATASET_NAME}.txt\")\n",
    "out_path.write_bytes(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
